{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"cnf-provisioning-low-latency-workloads/","title":"Provisioning real-time and low latency workloads","text":"<p>Many organizations need high performance computing and low, predictable latency, especially in the financial and telecommunications industries.</p> <p>OpenShift Container Platform provides the Node Tuning Operator to implement automatic tuning to achieve low latency performance and consistent response time for OpenShift Container Platform applications. You use the performance profile configuration to make these changes. You can update the kernel to kernel-rt, reserve CPUs for cluster and operating system housekeeping duties, including pod infra containers, isolate CPUs for application containers to run the workloads, and disable unused CPUs to reduce power consumption.</p> <p>Note</p> <p>When writing your applications, follow the general recommendations described in RHEL for Real Time processes and threads.</p>"},{"location":"cnf-provisioning-low-latency-workloads/#scheduling-a-low-latency-workload-onto-a-worker-with-real-time-capabilities","title":"Scheduling a low latency workload onto a worker with real-time capabilities","text":"<p>You can schedule low latency workloads onto a worker node where a performance profile that configures real-time capabilities is applied.</p> <p>Note</p> <p>To schedule the workload on specific nodes, use label selectors in the <code>Pod</code> custom resource (CR). The label selectors must match the nodes that are attached to the machine config pool that was configured for low latency by the Node Tuning Operator.</p> <ul> <li>You have installed the OpenShift CLI (<code>oc</code>).</li> <li>You have logged in as a user with <code>cluster-admin</code> privileges.</li> <li> <p>You have applied a performance profile in the cluster that tunes worker nodes for low latency workloads.</p> </li> <li> <p>Create a <code>Pod</code> CR for the low latency workload and apply it in the cluster, for example:</p> </li> </ul> <p>Example <code>Pod</code> spec configured to use real-time processing</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: dynamic-low-latency-pod\n  annotations:\n    cpu-quota.crio.io: \"disable\"\n    cpu-load-balancing.crio.io: \"disable\"\n    irq-load-balancing.crio.io: \"disable\"\nspec:\n  securityContext:\n    runAsNonRoot: true\n    seccompProfile:\n      type: RuntimeDefault\n  containers:\n  - name: dynamic-low-latency-pod\n    image: \"registry.redhat.io/openshift4/cnf-tests-rhel8:v{product-version}\"\n    command: [\"sleep\", \"10h\"]\n    resources:\n      requests:\n        cpu: 2\n        memory: \"200M\"\n      limits:\n        cpu: 2\n        memory: \"200M\"\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop: [ALL]\n  nodeSelector:\n    node-role.kubernetes.io/worker-cnf: \"\"\n  runtimeClassName: performance-dynamic-low-latency-profile \u2464\n# ...\n</code></pre> <ol> <li>Enter the pod <code>runtimeClassName</code> in the form performance-&lt;profile_name&gt;, where &lt;profile_name&gt; is the <code>name</code> from the <code>PerformanceProfile</code> YAML. In the previous example, the <code>name</code> is <code>performance-dynamic-low-latency-profile</code>.</li> <li> <p>Ensure the pod is running correctly. Status should be <code>running</code>, and the correct cnf-worker node should be set:</p> <pre><code>$ oc get pod -o wide\n</code></pre> <pre><code>NAME                     READY   STATUS    RESTARTS   AGE     IP           NODE\ndynamic-low-latency-pod  1/1     Running   0          5h33m   10.131.0.10  cnf-worker.example.com\n</code></pre> </li> <li> <p>Get the CPUs that the pod configured for IRQ dynamic load balancing runs on:</p> <pre><code>$ oc exec -it dynamic-low-latency-pod -- /bin/bash -c \"grep Cpus_allowed_list /proc/self/status | awk '{print $2}'\"\n</code></pre> <pre><code>Cpus_allowed_list:  2-3\n</code></pre> </li> </ol> <p>Ensure the node configuration is applied correctly.</p> <ol> <li> <p>Log in to the node to verify the configuration.</p> <pre><code>$ oc debug node/&lt;node-name&gt;\n</code></pre> </li> <li> <p>Verify that you can use the node file system:</p> <pre><code>sh-4.4# chroot /host\n</code></pre> <p><code>terminal  sh-4.4#</code></p> </li> <li> <p>Ensure the default system CPU affinity mask does not include the <code>dynamic-low-latency-pod</code> CPUs, for example, CPUs 2 and 3.</p> <pre><code>sh-4.4# cat /proc/irq/default_smp_affinity\n</code></pre> <pre><code>33\n</code></pre> </li> <li> <p>Ensure the system IRQs are not configured to run on the <code>dynamic-low-latency-pod</code> CPUs:</p> <pre><code>sh-4.4# find /proc/irq/ -name smp_affinity_list -exec sh -c 'i=\"$1\"; mask=$(cat $i); file=$(echo $i); echo $file: $mask' _ {} \\;\n</code></pre> <pre><code>/proc/irq/0/smp_affinity_list: 0-5\n/proc/irq/1/smp_affinity_list: 5\n/proc/irq/2/smp_affinity_list: 0-5\n/proc/irq/3/smp_affinity_list: 0-5\n/proc/irq/4/smp_affinity_list: 0\n/proc/irq/5/smp_affinity_list: 0-5\n/proc/irq/6/smp_affinity_list: 0-5\n/proc/irq/7/smp_affinity_list: 0-5\n/proc/irq/8/smp_affinity_list: 4\n/proc/irq/9/smp_affinity_list: 4\n/proc/irq/10/smp_affinity_list: 0-5\n/proc/irq/11/smp_affinity_list: 0\n/proc/irq/12/smp_affinity_list: 1\n/proc/irq/13/smp_affinity_list: 0-5\n/proc/irq/14/smp_affinity_list: 1\n/proc/irq/15/smp_affinity_list: 0\n/proc/irq/24/smp_affinity_list: 1\n/proc/irq/25/smp_affinity_list: 1\n/proc/irq/26/smp_affinity_list: 1\n/proc/irq/27/smp_affinity_list: 5\n/proc/irq/28/smp_affinity_list: 1\n/proc/irq/29/smp_affinity_list: 0\n/proc/irq/30/smp_affinity_list: 0-5\n</code></pre> <p>Warning</p> <p>When you tune nodes for low latency, the usage of execution probes in conjunction with applications that require guaranteed CPUs can cause latency spikes. Use other probes, such as a properly configured set of network probes, as an alternative.</p> </li> </ol>"},{"location":"cnf-provisioning-low-latency-workloads/#creating-a-pod-with-a-guaranteed-qos-class","title":"Creating a pod with a guaranteed QoS class","text":"<p>You can create a pod with a quality of service (QoS) class of <code>Guaranteed</code> for high-performance workloads. Configuring a pod with a QoS class of <code>Guaranteed</code> ensures that the pod has priority access to the specified CPU and memory resources.</p> <p>To create a pod with a QoS class of <code>Guaranteed</code>, you must apply the following specifications:</p> <ul> <li>Set identical values for the memory limit and memory request fields for each container in the pod.</li> <li>Set identical values for CPU limit and CPU request fields for each container in the pod.</li> </ul> <p>In general, a pod with a QoS class of <code>Guaranteed</code> will not be evicted from a node. One exception is during resource contention caused by system daemons exceeding reserved resources. In this scenario, the <code>kubelet</code> might evict pods to preserve node stability, starting with the lowest priority pods.</p> <ul> <li>Access to the cluster as a user with the <code>cluster-admin</code> role</li> <li> <p>The OpenShift CLI (<code>oc</code>)</p> </li> <li> <p>Create a namespace for the pod by running the following command:</p> <pre><code>$ oc create namespace qos-example \u2460\n</code></pre> </li> <li> <p>This example uses the <code>qos-example</code> namespace.</p> <p>Example output</p> <p><pre><code>namespace/qos-example created\n</code></pre> 2. Create the <code>Pod</code> resource:</p> </li> <li> <p>Create a YAML file that defines the <code>Pod</code> resource:</p> <pre><code>```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: qos-demo\n  namespace: qos-example\nspec:\n  securityContext:\n    runAsNonRoot: true\n    seccompProfile:\n      type: RuntimeDefault\n  containers:\n  - name: qos-demo-ctr\n    image: quay.io/openshifttest/hello-openshift:openshift \u2460\n    resources:\n      limits:\n        memory: \"200Mi\"\n        cpu: \"1\"\n      requests:\n        memory: \"200Mi\"\n        cpu: \"1\"\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop: [ALL]\n```\n</code></pre> <p>Note</p> <p>If you specify a memory limit for a container, but do not specify a memory request, OpenShift Container Platform automatically assigns a memory request that matches the limit. Similarly, if you specify a CPU limit for a container, but do not specify a CPU request, OpenShift Container Platform automatically assigns a CPU request that matches the limit.</p> </li> <li> <p>Create the <code>Pod</code> resource by running the following command:</p> <pre><code>$ oc apply -f qos-example.yaml --namespace=qos-example\n</code></pre> <p>Example output</p> <pre><code>pod/qos-demo created\n</code></pre> </li> <li> <p>View the <code>qosClass</code> value for the pod by running the following command:</p> <pre><code>$ oc get pod qos-demo --namespace=qos-example --output=yaml | grep qosClass\n</code></pre> <p>Example output</p> <pre><code>    qosClass: Guaranteed\n</code></pre> </li> </ul>"},{"location":"cnf-provisioning-low-latency-workloads/#disabling-cpu-load-balancing-in-a-pod","title":"Disabling CPU load balancing in a Pod","text":"<p>Functionality to disable or enable CPU load balancing is implemented on the CRI-O level. The code under the CRI-O disables or enables CPU load balancing only when the following requirements are met.</p> <ul> <li> <p>The pod must use the <code>performance-&lt;profile-name&gt;</code> runtime class. You can get the proper name by looking at the status of the performance profile, as shown here:</p> <pre><code>apiVersion: performance.openshift.io/v2\nkind: PerformanceProfile\n...\nstatus:\n  ...\n  runtimeClass: performance-manual\n</code></pre> </li> </ul> <p>The Node Tuning Operator is responsible for the creation of the high-performance runtime handler config snippet under relevant nodes and for creation of the high-performance runtime class under the cluster. It will have the same content as the default runtime handler except that it enables the CPU load balancing configuration functionality.</p> <p>To disable the CPU load balancing for the pod, the <code>Pod</code> specification must include the following fields:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  #...\n  annotations:\n    #...\n    cpu-load-balancing.crio.io: \"disable\"\n    #...\n  #...\nspec:\n  #...\n  runtimeClassName: performance-&lt;profile_name&gt;\n  #...\n</code></pre> <p>Note</p> <p>Only disable CPU load balancing when the CPU manager static policy is enabled and for pods with guaranteed QoS that use whole CPUs. Otherwise, disabling CPU load balancing can affect the performance of other containers in the cluster.</p>"},{"location":"cnf-provisioning-low-latency-workloads/#disabling-power-saving-mode-for-high-priority-pods","title":"Disabling power saving mode for high priority pods","text":"<p>You can configure pods to ensure that high priority workloads are unaffected when you configure power saving for the node that the workloads run on.</p> <p>When you configure a node with a power saving configuration, you must configure high priority workloads with performance configuration at the pod level, which means that the configuration applies to all the cores used by the pod.</p> <p>By disabling P-states and C-states at the pod level, you can configure high priority workloads for best performance and lowest latency.</p> Annotation Possible Values Description <code>cpu-c-states.crio.io:</code> * <code>\"enable\"</code> * <code>\"disable\"</code> * <code>\"max_latency:microseconds\"</code> This annotation allows you to enable or disable C-states for each CPU. Alternatively, you can also specify a maximum latency in microseconds for the C-states. For example, enable C-states with a maximum latency of 10 microseconds with the setting <code>cpu-c-states.crio.io</code>: <code>\"max_latency:10\"</code>. Set the value to <code>\"disable\"</code> to provide the best performance for a pod. <code>cpu-freq-governor.crio.io:</code> Any supported <code>cpufreq governor</code>. Sets the <code>cpufreq</code> governor for each CPU. The <code>\"performance\"</code> governor is recommended for high priority workloads. <ul> <li> <p>You have configured power saving in the performance profile for the node where the high priority workload pods are scheduled.</p> </li> <li> <p>Add the required annotations to your high priority workload pods. The annotations override the <code>default</code> settings.</p> </li> </ul> <p>Example high priority workload annotation</p> <p><pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  #...\n  annotations:\n    #...\n    cpu-c-states.crio.io: \"disable\"\n    cpu-freq-governor.crio.io: \"performance\"\n    #...\n  #...\nspec:\n  #...\n  runtimeClassName: performance-&lt;profile_name&gt;\n  #...\n</code></pre> 2. Restart the pods to apply the annotation.</p>"},{"location":"cnf-provisioning-low-latency-workloads/#disabling-cpu-cfs-quota","title":"Disabling CPU CFS quota","text":"<p>To eliminate CPU throttling for pinned pods, create a pod with the <code>cpu-quota.crio.io: \"disable\"</code> annotation. This annotation disables the CPU completely fair scheduler (CFS) quota when the pod runs.</p> <p>Example pod specification with <code>cpu-quota.crio.io</code> disabled</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  annotations:\n      cpu-quota.crio.io: \"disable\"\nspec:\n    runtimeClassName: performance-&lt;profile_name&gt;\n#...\n</code></pre> <p>Note</p> <p>Only disable CPU CFS quota when the CPU manager static policy is enabled and for pods with guaranteed QoS that use whole CPUs. For example, pods that contain CPU-pinned containers. Otherwise, disabling CPU CFS quota can affect the performance of other containers in the cluster.</p>"},{"location":"cnf-provisioning-low-latency-workloads/#disabling-interrupt-processing-for-cpus-where-pinned-containers-are-running","title":"Disabling interrupt processing for CPUs where pinned containers are running","text":"<p>To achieve low latency for workloads, some containers require that the CPUs they are pinned to do not process device interrupts. A pod annotation, <code>irq-load-balancing.crio.io</code>, is used to define whether device interrupts are processed or not on the CPUs where the pinned containers are running. When configured, CRI-O disables device interrupts where the pod containers are running.</p> <p>To disable interrupt processing for CPUs where containers belonging to individual pods are pinned, ensure that <code>globallyDisableIrqLoadBalancing</code> is set to <code>false</code> in the performance profile. Then, in the pod specification, set the <code>irq-load-balancing.crio.io</code> pod annotation to <code>disable</code>.</p> <p>The following pod specification contains this annotation:</p> <pre><code>apiVersion: performance.openshift.io/v2\nkind: Pod\nmetadata:\n  annotations:\n      irq-load-balancing.crio.io: \"disable\"\nspec:\n    runtimeClassName: performance-&lt;profile_name&gt;\n#...\n</code></pre>"},{"location":"concept-numa-scheduling-strategies/","title":"NUMA resource scheduling strategies","text":"<p>When scheduling high-performance workloads, the secondary scheduler can employ different strategies to determine which NUMA node within a chosen worker node will handle the workload. The supported strategies in OpenShift Container Platform include <code>LeastAllocated</code>, <code>MostAllocated</code>, and <code>BalancedAllocation</code>. Understanding these strategies helps optimize workload placement for performance and resource utilization.</p> <p>When a high-performance workload is scheduled in a NUMA-aware cluster, the following steps occur: </p> <ol> <li>The scheduler first selects a suitable worker node based on cluster-wide criteria. For example taints, labels, or resource availability.</li> <li>After a worker node is selected, the scheduler evaluates its NUMA nodes and applies a scoring strategy to decide which NUMA node will handle the workload.</li> <li>After a workload is scheduled, the selected NUMA node\u2019s resources are updated to reflect the allocation.</li> </ol> <p>Note</p> <p>The default strategy applied is the <code>LeastAllocated</code> strategy.  This assigns workloads to the NUMA node with the most available resources that is the least utilized NUMA node. The goal of this strategy is to spread workloads across NUMA nodes to reduce contention and avoid hotspots.</p> <p>The following table summarizes the different strategies and their outcomes:</p> <p>Scoring strategy summary</p> Strategy Description Outcome <code>LeastAllocated</code> Favors NUMA nodes with the most available resources. Spreads workloads to reduce contention and ensure headroom for high-priority tasks. <code>MostAllocated</code> Favors NUMA nodes with the least available resources. Consolidates workloads on fewer NUMA nodes, freeing others for energy efficiency. <code>BalancedAllocation</code> Favors NUMA nodes with balanced CPU and memory usage. Ensures even resource utilization, preventing skewed usage patterns."},{"location":"reference-root-device-hints/","title":"About root device hints","text":"<p>The <code>rootDeviceHints</code> parameter enables the installer to provision the {op-system-first} image to a particular device.  The installer examines the devices in the order it discovers them, and compares the discovered values with the hint values. The installer uses the first discovered device that matches the hint value. The configuration can combine multiple hints, but a device must match all hints for the installer to select it.</p> Subfield Description <code>deviceName</code> A string containing a Linux device name such as <code>/dev/vda</code> or <code>/dev/disk/by-path/</code>. It is recommended to use the <code>/dev/disk/by-path/&lt;device_path&gt;</code> link to the storage location. The hint must match the actual value exactly. <code>hctl</code> A string containing a SCSI bus address like <code>0:0:0:0</code>. The hint must match the actual value exactly. <code>model</code> A string containing a vendor-specific device identifier. The hint can be a substring of the actual value. <code>vendor</code> A string containing the name of the vendor or manufacturer of the device. The hint can be a sub-string of the actual value. <code>serialNumber</code> A string containing the device serial number. The hint must match the actual value exactly. <code>minSizeGigabytes</code> An integer representing the minimum size of the device in gigabytes. <code>wwn</code> A string containing the unique storage identifier. The hint must match the actual value exactly. If you use the <code>udevadm</code> command to retrieve the <code>wwn</code> value, and the command outputs a value for <code>ID_WWN_WITH_EXTENSION</code>, then you must use this value to specify the <code>wwn</code> subfield. <code>rotational</code> A boolean indicating whether the device should be a rotating disk (true) or not (false). <p>Example usage:</p> <pre><code>- name: master-0\n  role: master\n  rootDeviceHints:\n    deviceName: \"/dev/sda\"\n</code></pre>"},{"location":"task-installing-nro/","title":"Installing the NUMA Resources Operator using the CLI","text":"<p>You can install the Operator using the CLI.</p> <ol> <li> <p>Save the following YAML in the <code>nro-namespace.yaml</code> file and create the <code>Namespace</code> CR:</p> <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: openshift-numaresources\n</code></pre> <pre><code>$ oc create -f nro-namespace.yaml\n</code></pre> </li> <li> <p>Save the following YAML in the <code>nro-operatorgroup.yaml</code> file and create the <code>OperatorGroup</code> CR:</p> <pre><code>apiVersion: operators.coreos.com/v1\nkind: OperatorGroup\nmetadata:\n  name: numaresources-operator\n  namespace: openshift-numaresources\nspec:\n  targetNamespaces:\n  - openshift-numaresources\n</code></pre> <pre><code>oc create -f nro-operatorgroup.yaml\n</code></pre> </li> <li> <p>Save the following YAML in the <code>nro-sub.yaml</code> file and create the subscription for the NUMA Resources Operator:</p> <pre><code>apiVersion: operators.coreos.com/v1alpha1\nkind: Subscription\nmetadata:\n  name: numaresources-operator\n  namespace: openshift-numaresources\nspec:\n  channel: \"4.19\"\n  name: numaresources-operator\n  source: redhat-operators\n  sourceNamespace: openshift-marketplace\n</code></pre> <pre><code>oc create -f nro-sub.yaml\n</code></pre> </li> <li> <p>Verify that the installation succeeded by inspecting the CSV resource in the <code>openshift-numaresources</code> namespace. Run the following command:</p> </li> </ol> <pre><code>oc get csv -n openshift-numaresources\n</code></pre> <pre><code>NAME                             DISPLAY                  VERSION   REPLACES   PHASE\nnumaresources-operator.v4.19.2   numaresources-operator   4.19.2               Succeeded\n</code></pre>"}]}